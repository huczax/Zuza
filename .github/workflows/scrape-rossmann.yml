name: scrape-rossmann

on:
  workflow_dispatch:
  schedule:
    # 04:30 UTC = 06:30 PL latem, 05:30 zimą
    - cron: '30 4 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    container:
      image: mcr.microsoft.com/playwright/python:v1.54.0-jammy

    defaults:
      run:
        shell: bash

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Upgrade pip + install deps
        run: |
          python -m pip install --upgrade pip setuptools wheel
          if [ -f requirements.txt ]; then
            if grep -qE '^playwright' requirements.txt; then
              sed -i 's/^playwright.*/playwright==1.54.0/' requirements.txt
            else
              echo 'playwright==1.54.0' >> requirements.txt
            fi
            pip install -r requirements.txt
          fi

      - name: Install Playwright browsers + system deps
        run: |
          python -m playwright install --with-deps chromium

      - name: Sanity check Playwright
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          python - <<'PY'
          import asyncio
          from scraper.browser import browser_context
          from scraper.utils import safe_goto, network_settle
          async def t():
              async with browser_context(profile="desktop", headless=True) as ctx:
                  p = await ctx.new_page()
                  await safe_goto(p, "https://example.com")
                  await network_settle(p, 500)
                  print("TITLE:", await p.title())
          asyncio.run(t())
          PY

      - name: Smoke-test: zapisz screenshot do data/
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          python - <<'PY'
          import asyncio, os, datetime
          from scraper.browser import browser_context
          async def main():
              today = datetime.date.today().isoformat()
              out_dir = f"data/rossmann/{today}/images"
              os.makedirs(out_dir, exist_ok=True)
              async with browser_context(profile="desktop", headless=True) as ctx:
                  p = await ctx.new_page()
                  await p.goto("https://www.rossmann.pl", wait_until="domcontentloaded")
                  path = f"{out_dir}/smoke.png"
                  await p.screenshot(path=path, full_page=True)
                  print("Saved:", path, "size:", os.path.getsize(path), "bytes")
          asyncio.run(main())
          PY

      - name: Preflight: sprawdź DNS/HTTP do rossmann.pl
        run: |
          set -euxo pipefail
          getent hosts www.rossmann.pl || true
          curl -I --max-time 20 https://www.rossmann.pl/robots.txt || exit 1

      - name: Run scraper (debug) + list outputs
        env:
          LOGURU_LEVEL: DEBUG
          PYTHONUNBUFFERED: "1"
        run: |
          set -euo pipefail
          python -m scraper.runner --site rossmann --profiles desktop,mobile --headless true --save-snapshots true
          echo "--- DATA DIR ---"
          ls -R data || true
          echo "--- FILES FOUND ---"
          find . -type f | sort
          echo "--- HEALTH REPORT (if present) ---"
          if ls data/rossmann/*/health_report.json >/dev/null 2>&1; then
            sed -n '1,200p' data/rossmann/*/health_report.json
          fi

      - name: List outputs (debug)
        if: always()
        run: |
          ls -R data || true
          find data -type f | sort || true

      - name: Pack data & traces (smaller upload)
        if: always()
        run: |
          TODAY=$(TZ=Europe/Warsaw date +%F)
          mkdir -p out
          tar -czf out/data-$TODAY.tar.gz -C data .
          if [ -d traces ]; then
            tar -czf out/traces-$TODAY.tar.gz -C traces .
          fi
          ls -lh out

      - name: Upload run artifacts (90 days)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rossmann-${{ github.run_id }}
          path: |
            out/*.tar.gz
            data/rossmann/**
          if-no-files-found: warn
          retention-days: 90


      - name: Assert snapshots & health report exist
        run: |
          set -euo pipefail
          D="data/rossmann/$(date -u +%F)"   # scraper używa dzisiejszej daty
          test -d "$D/snapshots" || { echo "Brak katalogu snapshots w $D"; exit 1; }
          # co najmniej 1 plik w snapshots:
          ls -1 "$D/snapshots"/* >/dev/null 2>&1 || { echo "Brak plików w snapshots"; exit 1; }
          # health_report.json:
          test -s "$D/health_report.json" || { echo "Brak lub pusty health_report.json"; exit 1; }

      - name: Upload artifact (data/)
        uses: actions/upload-artifact@v4
        with:
          name: data-${{ github.run_id }}
          path: data/**
          if-no-files-found: warn
